<!DOCTYPE html>
<html>
<head>
    <title>Learning From Music to Visual Storytelling of Shots: A Deep Interactive Learning Mechanism</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: 'Microsoft Yahei' !important;
        }
        h1 {
            text-align: center;
            color: #aae0ee; /* 水藍色 */
        }
        h2 {
            text-align: center;
		color: #e6eced;
        }
        h3 {
            text-align: center;
        }
        p {
            text-align: center;
        }
        hr {
      width: 65%; /* 或者使用百分比，例如 50% */
      <!--border: 1px solid #000; /* 可選，設定邊框樣式 */-->
    }
        .example {
    max-width: 1000px;
    margin: 0 auto;
    padding: 20px;
  }
    </style>
</head>
<body>

<h1>Learning From Music to Visual Storytelling of Shots: A Deep Interactive Learning Mechanism</h1>
	<br>
	<h3>Jen-Chun Lin, Wen-Li Wei, Yen-Yu Lin, Tyng-Luh Liu, Hong-Yuan Mark Liao</h3>	
<h3>Institute of Information Science, Academia Sinica, Taiwan</h3>
	<h3>ACM Multimedia 2020</h3>
	<br>
	
	<hr>
<h2>Music to visual storytelling of shots</h2>
	<br>
	
<div style="text-align: center;">
  <video controls>    
    <source src="Ed Sheeran - Supermarket Flowers [Live from the BRITs 2018]-cut.mp4" type="video/mp4">
  </video>
</div>
<p>An example of visual storytelling for the song “Supermarket Flowers” by Ed Sheeran live at the BRIT Awards 2018. </p>
    <p>The director sequentially uses the XLS, MCU, XLS, and MS to expand the storytelling potential in the beginning of the song.</p>
<br>
	<div style="text-align: center;">
  <img src="storytelling_resize.png"  width="1000">
</div>
	<br>
<hr>
    <h2>Types of Shots</h2>
    <div class="example">
      In the language of film, shot (type) is a fundamental element of visual storytelling [1, 2]. The type of shot is defined as how much a target subject and its surrounding area can be seen. Totally, six types of shots are defined, as described in the following Table.
    </div>
    
    <div style="text-align: center;">
  <img src="icme2TMM_table1.png"  width="1000">
</div>
	
<div class="example">
      Besides the six types of shots, the audience shot (ADS) and musical instrument shot (MIS) also considered to enrich the visual storytelling in a concert video.
    </div>
	<br>
<hr>
<h2>Proposed Deep Interactive Learning Mechanism</h2>
<div class="example">
	<br>In this study, we present a deep interactive learning (DIL) mechanism for building a compact yet accurate sequence-to-sequence model to achieve the music to visual storytelling of shots translation. 
</br>
<br>Different from the one-way transfer between a pre-trained teacher network (or ensemble network) and a student network in knowledge distillation (KD) [3], the proposed DIL mechanism enables collaborative learning between an ensemble teacher network and a student network. Namely, the student network also teaches.
</br>
<br>In the proposed DIL mechanism, learning comes from two aspects: teacher-to-student and student-to-teacher. 
	</br>
<br>Regarding <font color="yellow">teacher-to-student</font> learning, the process starts with a powerful pre-trained teacher network, MF-RNNs with film-language [4], and then performs knowledge transfer to a student network (lightweight RNN) through KD.</br>
<br><div style="text-align: center;"><img src="KD.png"></div></br>
	<br>In <font color="yellow">student-to-teacher</font> learning, we turn to distill the knowledge (soft target distribution) from the student network and then transfer the knowledge to each of assistant networks (temporal resolution RNNs) integrated in the teacher network, thereby upgrading the teacher network.</br>
<br><div style="text-align: center;"><img src="stu2tch.png" width="1000"></div></br>

	<br>Thus, repeating such a DIL mechanism gradually improves the performance of both student and teacher networks.</br>

<br>After training, the student network is finally used to achieve the music to visual storytelling of shots translation.</br>
</div>

	<br>	
	<hr>
<h2>Demo</h2>
	<br>Subjective evaluation in terms of 5-point mean opinion score (MOS) is conducted on three concert video sets. For a concert video set, each concert video is generated from multiple audience recordings that under the guidance of official shot type sequence and the shot type sequences translated from lightweight RNN-DIL, MF-RNNs-DIL, lightweight RNN-KD [1], and MF-RNNs (with film-language) [2], respectively.</br>

<br>Three indicators are considered for evaluation: (1) Does the frequency of shot switching match the music? (2) Does the timing of cut point match the music? (3) Overall, does the visual storytelling of shots match the music?</br>

<br>For each figure, the vertical and horizontal axes represent the number of people and MOS score (from bad to excellent), respectively.</br>
	<br>
	<hr>
<h2>Demo</h2>
	<br>
<hr>
 	  <a name="related_work"></a>
   	<table align=center width=1100px>
 	      <tr>
  	      	<td width=400px>
 		           	<left>
  	   		    	<center><h2>References</h2></center>
 		 	   	      <br> [1] D. Andrews. Communications & Multimedia Technology. Digital Overdrive, 2011. 
			   	    	<br> [2] G. Mercado. The Filmmaker's Eye: Learning (and Breaking) the Rules of Cinematic Composition. Taylor & Francis, 2010.
					<br> [3] G. E. Hinton, O. Vinyals, and J. Dean, Distilling the knowledge in a neural network, In NeurIPS Workshop, 2015.
				<br>[4] W.-L. Wei, J.-C. Lin, T.-L. Liu, Y.-H. Yang, H.-M. Wang, H.-R. Tyan, and H.-Y. Liao, Seethevoice: Learning from music to visual storytelling of shots, ICME, 2018.</br>
                    </left>
           </td>
	      </tr>
  	</table>
    
</body>
</html>
